# Visualizando e acessando os coeficientes obtidos com o modelo
modelo_ML
# O que existe no modelo de ML?
names(modelo_ML)
# Coeficientes
modelo_ML$coefficients[1]
modelo_ML$coefficients[2]
# Resíduos do modelo (diferença entre o modelo e os dados de treinamento)
modelo_ML$residuals
# Obtenha a média dos resíduos
media_residuos = mean(modelo_ML$residuals)
media_residuos
# Gerando predições
?predict
previsao_treinamento = predict(modelo_ML)
class(previsao_treinamento)
previsao_treinamento = as.data.frame(previsao_treinamento)
View(previsao_treinamento)
class(previsao_treinamento)
# Plotando os dados de treinamento
plot(data$Population,data$Profit, main = 'Gráfico de Dispersão - População vs Lucro',
xlab = 'População (baseado 10,000 hab)', ylab = 'Lucro (baseado em $10,000 dólares)', pch = 1, col = 'blue')
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
grid()
# Plotando a linha de regressão
?abline
# dica: função abline(a,b)
# 1) a -> intercept do modelo de regressão
# 2) b -> coeficiente da reta do modelo de regressão
abline(modelo_ML$coefficients[1],modelo_ML$coefficients[2],col = 'red')
modelo_ML$coefficients[1]
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=3, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(predicao_teste*10000)
names(predicao_teste) <- c('Predições (dólares)')
predicao_teste
# ------------------------------------------------------------------------------------------------------
# Implementação do algoritmo do gradiente descendente para obtenção do mesmo resultado da função lm do R
theta0 = 0
theta1 = 0
theta  = c(theta0,theta1)
# Criando funções
Cost_computation <- function(x, y, theta){
# Verificação do número de exemplos de treinamento
m = length(y)
# Inicialização do Custo
J = 0
# Cômputo do custo a partir das informações fornecidas:
# i)   matriz de design
# ii)  rótulos ou respostas
# iii) parâmetros inicializados
# Parâmetros - de acordo com o modelo de regressão linear
Theta0 = theta[1]
Theta1 = theta[2]
# Função hipótese candidata de acordo com o modelo linear
h = Theta0 + Theta1*x
# Cômputo do custo (repare na versão vetorizada com Matlab)
Cost = sum((h - y)^2)
# Ponderação do custo pela quantidade de exemplos de treinamento
J = (1/(2*m))*Cost
}
# Cálculo da função custo para esses valores do vetor de parâmetros theta
Custo = Cost_computation(x,y,theta)
Custo
# Inicializações relacionada ao gradiente descendente
num_iters     = 15000;
learning_rate = 0.01;
Algoritmo_GD <- function(X, y, theta, learning_rate, num_iters){
# Verificação do número de exemplos de treinamento
m = length(y)
# Uso da variável alpha = taxa de aprendizagem
alpha = learning_rate
# Loop para iterações do algoritmo GD
for (i in 1:num_iters){
# ================================================================================
h      = theta[1] + theta[2]*x                   # Função hipótese
Theta0 = theta[1]                                # Parâmetro (bias)
Theta1 = theta[2]                                # Parâmetro da característica
Theta0 = Theta0 - alpha*(1/m)*sum((h - y))       # Algoritmo GD (theta 0)
Theta1 = Theta1 - alpha*(1/m)*sum((h - y)*x)     # Algoritmo GD (theta 1)
theta  = c(Theta0, Theta1)                       # Composição de vetor de parâmetro
# print(theta)
}
# ================================================================================
theta
}
theta_GD <- Algoritmo_GD(x, y, theta, learning_rate, num_iters)
theta_GD
# Característica unitária
size_data = dim(data)
ones_data = replicate(size_data[1],1)
# Matriz de Design
X = data.frame(ones_data,x)
X = as.matrix(X)
View(X)
theta_solution_norm_equations = solve(t(X) %*% X) %*% (t(X) %*% y)
theta_0_norm_eq = theta_solution_norm_equations[1]
theta_1_norm_eq = theta_solution_norm_equations[2]
theta_0_norm_eq
theta_1_norm_eq
# Summary do modelo - avaliando sua performance e detalhes
summary(modelo_ML)
# Equação de Regressão
# Equação de Regressão
# y = a + bx (simples)
# Resíduos
# Resíduos
# Diferença entre os valores observados de uma variável e seus valores previstos
# Resíduos
# Diferença entre os valores observados de uma variável e seus valores previstos
# Seus resíduos devem se parecer com uma distribuição normal, o que indica
# Coeficiente - Intercept - a (alfa)
# Coeficientes - Nomes das variáveis - b (beta)
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# rotulagem para cada variável explicativa.
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# rotulagem para cada variável explicativa.
# Em vez de escrever slope_1, slope_2, ....
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# rotulagem para cada variável explicativa.
# Em vez de escrever slope_1, slope_2, ....
# Eles simplesmente usam o nome da variável em qualquer saída para
x <- data$Population
y <- data$Profit
x <- data$Population
y <- data$Profit
# Plotando os dados de treinamento
plot(data$Population,data$Profit, main = 'Gráfico de Dispersão - População vs Lucro',
xlab = 'População (baseado 10,000 hab)', ylab = 'Lucro (baseado em $10,000 dólares)', pch = 1, col = 'blue')
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
grid()
# Plotando a linha de regressão
?abline
# dica: função abline(a,b)
# 1) a -> intercept do modelo de regressão
# 2) b -> coeficiente da reta do modelo de regressão
abline(modelo_ML$coefficients[1],modelo_ML$coefficients[2],col = 'red')
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=3, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=1, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=1, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=1.5, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(predicao_teste*10000)
names(predicao_teste) <- c('Predições (dólares)')
predicao_teste
# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(novo_dado, predicao_teste*10000)
names(predicao_teste) <- c('População','Predições (dólares)')
predicao_teste
# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(novo_dado*10000, predicao_teste*10000)
names(predicao_teste) <- c('População','Predições (dólares)')
predicao_teste
# Implementação do algoritmo do gradiente descendente para obtenção do mesmo resultado da função lm do R
theta0 = 0
theta1 = 0
theta  = c(theta0,theta1)
# Criando funções
Cost_computation <- function(x, y, theta){
# Verificação do número de exemplos de treinamento
m = length(y)
# Inicialização do Custo
J = 0
# Cômputo do custo a partir das informações fornecidas:
# i)   matriz de design
# ii)  rótulos ou respostas
# iii) parâmetros inicializados
# Parâmetros - de acordo com o modelo de regressão linear
Theta0 = theta[1]
Theta1 = theta[2]
# Função hipótese candidata de acordo com o modelo linear
h = Theta0 + Theta1*x
# Cômputo do custo (repare na versão vetorizada com Matlab)
Cost = sum((h - y)^2)
# Ponderação do custo pela quantidade de exemplos de treinamento
J = (1/(2*m))*Cost
}
# Cálculo da função custo para esses valores do vetor de parâmetros theta
Custo = Cost_computation(x,y,theta)
Custo
# Algoritmo do Gradiente Descendente ------------------------------------------------------------
# Inicializações relacionada ao gradiente descendente
num_iters     = 15000;
learning_rate = 0.01;
Algoritmo_GD <- function(X, y, theta, learning_rate, num_iters){
# Verificação do número de exemplos de treinamento
m = length(y)
# Uso da variável alpha = taxa de aprendizagem
alpha = learning_rate
# Loop para iterações do algoritmo GD
for (i in 1:num_iters){
# ================================================================================
h      = theta[1] + theta[2]*x                   # Função hipótese
Theta0 = theta[1]                                # Parâmetro (bias)
Theta1 = theta[2]                                # Parâmetro da característica
Theta0 = Theta0 - alpha*(1/m)*sum((h - y))       # Algoritmo GD (theta 0)
Theta1 = Theta1 - alpha*(1/m)*sum((h - y)*x)     # Algoritmo GD (theta 1)
theta  = c(Theta0, Theta1)                       # Composição de vetor de parâmetro
# print(theta)
}
# ================================================================================
theta
}
theta_GD <- Algoritmo_GD(x, y, theta, learning_rate, num_iters)
theta_GD
# ===============================================================================================
# Uso das equações normais para solução da regressão linear simples
# Repare que nós usamos a matriz de design com a característica unitária,
# pois queremos encontrar o parâmetro theta_0 ou intercept do modelo
# Característica unitária
size_data = dim(data)
ones_data = replicate(size_data[1],1)
# Matriz de Design
X = data.frame(ones_data,x)
X = as.matrix(X)
View(X)
theta_solution_norm_equations = solve(t(X) %*% X) %*% (t(X) %*% y)
theta_0_norm_eq = theta_solution_norm_equations[1]
theta_1_norm_eq = theta_solution_norm_equations[2]
theta_0_norm_eq
theta_1_norm_eq
summary(modelo_ML)
# ===============================================================================================
# Descrição detalhada do summary do modelo ------------------------------------------------------
# Summary do modelo - avaliando sua performance e detalhes
summary(modelo_ML)
# Equação de Regressão
# y = a + bx (simples)
# y = a + b0x0 + b1x1 (múltipla)
# Resíduos
# Diferença entre os valores observados de uma variável e seus valores previstos
# Seus resíduos devem se parecer com uma distribuição normal, o que indica
# que a média entre os valores previstos e os valores observados é próximo de 0 (o que é bom)
# Coeficiente - Intercept - a (alfa)
# Valor de a na equação de regressão
# Coeficientes - Nomes das variáveis - b (beta)
# Valor de b na equação de regressão
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# rotulagem para cada variável explicativa.
# Em vez de escrever slope_1, slope_2, ....
# Eles simplesmente usam o nome da variável em qualquer saída para
# indicar quais coeficientes pertencem a qual variável.
# Erro Padrão
# Medida de variabilidade na estimativa do coeficiente a (alfa). O ideal é que este valor
# seja menor que o valor do coeficiente, mas nem sempre isso irá ocorrer.
# Asteriscos
# Os asteriscos representam os níveis de significância de acordo com o p-value.
# Quanto mais estrelas, maior a significância.
# Atenção --> Muitos astericos indicam que é improvável que não exista
# relacionamento entre as variáveis.
# Valor t
# Define se coeficiente da variável é significativo ou não para o modelo.
# Ele é usado para calcular o p-value e os níveis de significância.
# p-value
# O p-value representa a probabilidade que a variável não seja relevante.
# Deve ser o menor valor possível.
# Se este valor for realmente pequeno, o R irá mostrar o valor
# como notação científica
# Significância
# São aquelas legendas próximas as suas variáveis
# Espaço em branco - ruim
# Pontos - razoável
# Asteriscos - bom
# Muitos asteriscos - muito bom
# Residual Standar Error
# Este valor representa o desvio padrão dos resíduos
# Degrees of Freedom
# É a diferença entre o número de observações na amostra de treinamento
# e o número de variáveis no seu modelo
# R-squared (coeficiente de determinação - R^2)
# Ajuda a avaliar o nível de precisão do nosso modelo.
# Quanto maior, melhor, sendo 1 o valor ideal.
# F-statistics
# É o teste F do modelo. Esse teste obtém os parâmetros do nosso modelo
# e compara com um modelo que tenha menos parâmetros.
# Em teoria, um modelo com mais parâmetros tem um desempenho melhor.
# Se o seu modelo com mais parâmetros NÃO tiver perfomance
# melhor que um modelo com menos parâmetros, o valor do p-value será bem alto.
# Se o modelo com mais parâmetros tiver performance
# melhor que um modelo com menos parâmetros, o valor do p-value será mais baixo.
# Lembre-se que correlação não implica causalidade
- CRIM: per capita crime rate by town
- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.
- INDUS: proportion of non-retail business acres per town
- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
- NOX: nitric oxides concentration (parts per 10 million)
- RM: average number of rooms per dwelling
- AGE: proportion of owner-occupied units built prior to 1940
- DIS: weighted distances to five Boston employment centres
- RAD: index of accessibility to radial highways
- TAX: full-value property-tax rate per 10,000
- PTRATIO: pupil-teacher ratio by town
- B: 1000(Bk􀀀0:63)2 where Bk is the proportion of blacks by town
- LSTAT: % lower status of the population
- TARGET: Median value of owner-occupied homes in $1000’sEm um primeiro momento, vamos usar a variável explanatória lstat, a qual expressa a parcela (em %) da
população de baixa renda (status) obtida em cada vizinhança dentre as 506 analisadas.
O nosso objetivo consiste em obter um modelo de ML baseado na regressão linear simples e univariada de
lstat e os valores medianos dos preços das casas medv. Abaixo, seguem os itens que devemos solucionar neste
desenvolvimento, visando alcançar o objetivo deste exercício:
 Questões Avaliativas
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
data
data
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
?Boston
data <- Boston
data
data <- Boston
library(MASS)
?Boston
data <- Boston
data
data <- Boston
data
head(data)
head(data)
install.packages(c("backports", "broom", "callr", "car", "carData", "dbplyr", "dplyr", "ellipsis", "foreach", "forecast", "fs", "ggplot2", "glue", "haven", "httpuv", "isoband", "janitor", "later", "lme4", "lubridate", "maptools", "modelr", "openxlsx", "padr", "pillar", "pkgbuild", "pkgload", "plotly", "pROC", "promises", "ps", "purrr", "quantmod", "Rcpp", "RcppArmadillo", "recipes", "reshape2", "rlang", "rmarkdown", "scales", "seasonal", "shinyWidgets", "sp", "SQUAREM", "tibble", "tibbletime", "tidyr", "tidyselect", "timetk", "tinytex", "tripack", "vctrs", "withr", "xfun", "xml2", "zoo"))
knitr::opts_chunk$set(echo = TRUE)
data <- Boston
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
knitr::opts_chunk$set(echo = TRUE)
plot(cars)
library(MASS)
?Boston
summary(Boston)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
# Utiliza funções gráficas simples para verificar a relação entre variáveis:
# ?plot
plot(data$lstat, data$medv, main = 'Gráfico de Dispersão - lstat vs medv',
xlab = 'lstat', ylab = 'Preço mediano da casa', pch = 1, col = 'blue')
# ?legend
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
# ?grid
grid()
modelo_ML <- lm(medv ~ lstat, data)
x <- data$lstat
y <- data$medv
summary(modelo_ML)
# Coeficientes
modelo_ML$coefficients[1]
modelo_ML$coefficients[2]
# Resíduos do modelo (diferença entre o modelo e os dados de treinamento)
modelo_ML$residuals
# Obtenha a média dos resíduos
media_residuos = mean(modelo_ML$residuals)
media_residuos
previsao_treinamento <- predict(modelo_ML) %>% as.data.frame()
library(MASS)
library(rlang)
library(tidyverse)
install.packages(rlang)
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rlang)
library(tidyverse)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rlang)
library(tidyverse)
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
saveRDS(df,'C:\Users\aganh\Documents\Ciência de dados e decisão\Exercícios computacionais\cd-regressao-linear\.RData')
saveRDS(df,'C:\\Users\\aganh\\Documents\\Ciência de dados e decisão\\Exercícios computacionais\\cd-regressao-linear\\.RData')
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
