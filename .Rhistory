summary(data)
# Utiliza funções gráficas simples para verificar a relação entre variáveis: população (population) e lucro (profit)
# ?plot
plot(data$Population, data$Profit, main = 'Gráfico de Dispersão - População vs Lucro',
xlab = 'População', ylab = 'Lucro', pch = 1, col = 'blue')
# ?legend
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
# ?grid
grid()
modelo_ML <- lm(profit ~ population, data = data)
# Plotando os dados de treinamento
plot(data$Population,data$Profit, main = 'Gráfico de Dispersão - População vs Lucro',
xlab = 'População (baseado 10,000 hab)', ylab = 'Lucro (baseado em $10,000 dólares)', pch = 1, col = 'blue')
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
grid()
# Plotando a linha de regressão
?abline
# dica: função abline(a,b)
# 1) a -> intercept do modelo de regressão
# 2) b -> coeficiente da reta do modelo de regressão
abline(modelo_ML$coefficients[1],modelo_ML$coefficients[2],col = 'red')
# Plotando a linha de regressão
?abline
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=3, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(predicao_teste*10000)
names(predicao_teste) <- c('Predições (dólares)')
predicao_teste
# ------------------------------------------------------------------------------------------------------
# Implementação do algoritmo do gradiente descendente para obtenção do mesmo resultado da função lm do R
theta0 = 0
theta1 = 0
theta  = c(theta0,theta1)
# Criando funções
Cost_computation <- function(x, y, theta){
# Verificação do número de exemplos de treinamento
m = length(y)
# Inicialização do Custo
J = 0
# Cômputo do custo a partir das informações fornecidas:
# i)   matriz de design
# ii)  rótulos ou respostas
# iii) parâmetros inicializados
# Parâmetros - de acordo com o modelo de regressão linear
Theta0 = theta[1]
Theta1 = theta[2]
# Função hipótese candidata de acordo com o modelo linear
h = Theta0 + Theta1*x
# Cômputo do custo (repare na versão vetorizada com Matlab)
Cost = sum((h - y)^2)
# Ponderação do custo pela quantidade de exemplos de treinamento
J = (1/(2*m))*Cost
}
# Cálculo da função custo para esses valores do vetor de parâmetros theta
Custo = Cost_computation(x,y,theta)
Custo
# Inicializações relacionada ao gradiente descendente
num_iters     = 15000;
learning_rate = 0.01;
Algoritmo_GD <- function(X, y, theta, learning_rate, num_iters){
# Verificação do número de exemplos de treinamento
m = length(y)
# Uso da variável alpha = taxa de aprendizagem
alpha = learning_rate
# Loop para iterações do algoritmo GD
for (i in 1:num_iters){
# ================================================================================
h      = theta[1] + theta[2]*x                   # Função hipótese
Theta0 = theta[1]                                # Parâmetro (bias)
Theta1 = theta[2]                                # Parâmetro da característica
Theta0 = Theta0 - alpha*(1/m)*sum((h - y))       # Algoritmo GD (theta 0)
Theta1 = Theta1 - alpha*(1/m)*sum((h - y)*x)     # Algoritmo GD (theta 1)
theta  = c(Theta0, Theta1)                       # Composição de vetor de parâmetro
# print(theta)
}
# ================================================================================
theta
}
theta_GD <- Algoritmo_GD(x, y, theta, learning_rate, num_iters)
theta_GD
# Característica unitária
size_data = dim(data)
ones_data = replicate(size_data[1],1)
# Matriz de Design
X = data.frame(ones_data,x)
X = as.matrix(X)
View(X)
theta_solution_norm_equations = solve(t(X) %*% X) %*% (t(X) %*% y)
theta_0_norm_eq = theta_solution_norm_equations[1]
theta_1_norm_eq = theta_solution_norm_equations[2]
theta_0_norm_eq
theta_1_norm_eq
# Summary do modelo - avaliando sua performance e detalhes
summary(modelo_ML)
# Equação de Regressão
# Equação de Regressão
# y = a + bx (simples)
# Resíduos
# Resíduos
# Diferença entre os valores observados de uma variável e seus valores previstos
# Resíduos
# Diferença entre os valores observados de uma variável e seus valores previstos
# Seus resíduos devem se parecer com uma distribuição normal, o que indica
# Coeficiente - Intercept - a (alfa)
source('~/Documents/Ciência de Dados e Decisões/Aula 2/Exercício Computacional - 1/R/Exercicio_Computacional_1.R', echo=TRUE)
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=3, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=3, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
source('~/Documents/Ciência de Dados e Decisões/Aula 2/Exercício Computacional - 1/R/Exercicio_Computacional_1.R', echo=TRUE)
data
class(data)    # Toda vez que importarmos algum conjunto de dados para o R, verifique sua classe (data.frame)
dim(data)      # Verificar a dimensão do objeto data
View(data)     # Use essa instrução para visualizar os dados (tal como um spreedcheat do excel)
# Nomeando as colunas de um dataframe - vamos usar a instrução names para colocar nomes nas colunas
names(data) <- c("Population","Profit")
head(data)
View(data)
# Utiliza funções gráficas simples para verificar a relação entre variáveis: população (population) e lucro (profit)
# ?plot
plot(data$Population, data$Profit, main = 'Gráfico de Dispersão - População vs Lucro',
xlab = 'População', ylab = 'Lucro', pch = 1, col = 'blue')
# ?legend
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
# ?grid
grid()
# Menor lucro
min(data$Profit)
max(data$Profit)
# Construção de Camadas com ggplot2
objeto_ggplot <- ggplot(data,
aes( x = Population, y = Profit)) +
xlab('População (baseada em 10.000 habitantes)')+
ylab('Lucro (baseado em $10,000 dólares)')
# Criação de uma camada de pontos
camada_1 <- geom_point()
# Geração do gráfico a adição da camada 1
objeto_ggplot + camada_1
modelo_ML <- lm(Profit ~ Population, data = data)
modelo_ML
x = data$Population
y = data$Profit
# Visualizando e acessando os coeficientes obtidos com o modelo
modelo_ML
# O que existe no modelo de ML?
names(modelo_ML)
# Coeficientes
modelo_ML$coefficients[1]
modelo_ML$coefficients[2]
# Resíduos do modelo (diferença entre o modelo e os dados de treinamento)
modelo_ML$residuals
# Obtenha a média dos resíduos
media_residuos = mean(modelo_ML$residuals)
media_residuos
# Gerando predições
?predict
previsao_treinamento = predict(modelo_ML)
class(previsao_treinamento)
previsao_treinamento = as.data.frame(previsao_treinamento)
View(previsao_treinamento)
class(previsao_treinamento)
# Plotando os dados de treinamento
plot(data$Population,data$Profit, main = 'Gráfico de Dispersão - População vs Lucro',
xlab = 'População (baseado 10,000 hab)', ylab = 'Lucro (baseado em $10,000 dólares)', pch = 1, col = 'blue')
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
grid()
# Plotando a linha de regressão
?abline
# dica: função abline(a,b)
# 1) a -> intercept do modelo de regressão
# 2) b -> coeficiente da reta do modelo de regressão
abline(modelo_ML$coefficients[1],modelo_ML$coefficients[2],col = 'red')
modelo_ML$coefficients[1]
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=3, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(predicao_teste*10000)
names(predicao_teste) <- c('Predições (dólares)')
predicao_teste
# ------------------------------------------------------------------------------------------------------
# Implementação do algoritmo do gradiente descendente para obtenção do mesmo resultado da função lm do R
theta0 = 0
theta1 = 0
theta  = c(theta0,theta1)
# Criando funções
Cost_computation <- function(x, y, theta){
# Verificação do número de exemplos de treinamento
m = length(y)
# Inicialização do Custo
J = 0
# Cômputo do custo a partir das informações fornecidas:
# i)   matriz de design
# ii)  rótulos ou respostas
# iii) parâmetros inicializados
# Parâmetros - de acordo com o modelo de regressão linear
Theta0 = theta[1]
Theta1 = theta[2]
# Função hipótese candidata de acordo com o modelo linear
h = Theta0 + Theta1*x
# Cômputo do custo (repare na versão vetorizada com Matlab)
Cost = sum((h - y)^2)
# Ponderação do custo pela quantidade de exemplos de treinamento
J = (1/(2*m))*Cost
}
# Cálculo da função custo para esses valores do vetor de parâmetros theta
Custo = Cost_computation(x,y,theta)
Custo
# Inicializações relacionada ao gradiente descendente
num_iters     = 15000;
learning_rate = 0.01;
Algoritmo_GD <- function(X, y, theta, learning_rate, num_iters){
# Verificação do número de exemplos de treinamento
m = length(y)
# Uso da variável alpha = taxa de aprendizagem
alpha = learning_rate
# Loop para iterações do algoritmo GD
for (i in 1:num_iters){
# ================================================================================
h      = theta[1] + theta[2]*x                   # Função hipótese
Theta0 = theta[1]                                # Parâmetro (bias)
Theta1 = theta[2]                                # Parâmetro da característica
Theta0 = Theta0 - alpha*(1/m)*sum((h - y))       # Algoritmo GD (theta 0)
Theta1 = Theta1 - alpha*(1/m)*sum((h - y)*x)     # Algoritmo GD (theta 1)
theta  = c(Theta0, Theta1)                       # Composição de vetor de parâmetro
# print(theta)
}
# ================================================================================
theta
}
theta_GD <- Algoritmo_GD(x, y, theta, learning_rate, num_iters)
theta_GD
# Característica unitária
size_data = dim(data)
ones_data = replicate(size_data[1],1)
# Matriz de Design
X = data.frame(ones_data,x)
X = as.matrix(X)
View(X)
theta_solution_norm_equations = solve(t(X) %*% X) %*% (t(X) %*% y)
theta_0_norm_eq = theta_solution_norm_equations[1]
theta_1_norm_eq = theta_solution_norm_equations[2]
theta_0_norm_eq
theta_1_norm_eq
# Summary do modelo - avaliando sua performance e detalhes
summary(modelo_ML)
# Equação de Regressão
# Equação de Regressão
# y = a + bx (simples)
# Resíduos
# Resíduos
# Diferença entre os valores observados de uma variável e seus valores previstos
# Resíduos
# Diferença entre os valores observados de uma variável e seus valores previstos
# Seus resíduos devem se parecer com uma distribuição normal, o que indica
# Coeficiente - Intercept - a (alfa)
# Coeficientes - Nomes das variáveis - b (beta)
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# rotulagem para cada variável explicativa.
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# rotulagem para cada variável explicativa.
# Em vez de escrever slope_1, slope_2, ....
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# rotulagem para cada variável explicativa.
# Em vez de escrever slope_1, slope_2, ....
# Eles simplesmente usam o nome da variável em qualquer saída para
x <- data$Population
y <- data$Profit
x <- data$Population
y <- data$Profit
# Plotando os dados de treinamento
plot(data$Population,data$Profit, main = 'Gráfico de Dispersão - População vs Lucro',
xlab = 'População (baseado 10,000 hab)', ylab = 'Lucro (baseado em $10,000 dólares)', pch = 1, col = 'blue')
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
grid()
# Plotando a linha de regressão
?abline
# dica: função abline(a,b)
# 1) a -> intercept do modelo de regressão
# 2) b -> coeficiente da reta do modelo de regressão
abline(modelo_ML$coefficients[1],modelo_ML$coefficients[2],col = 'red')
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=3, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=1, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=1, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=1.5, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gráfico de Dispersão - População vs Lucro")
# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(predicao_teste*10000)
names(predicao_teste) <- c('Predições (dólares)')
predicao_teste
# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(novo_dado, predicao_teste*10000)
names(predicao_teste) <- c('População','Predições (dólares)')
predicao_teste
# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(novo_dado*10000, predicao_teste*10000)
names(predicao_teste) <- c('População','Predições (dólares)')
predicao_teste
# Implementação do algoritmo do gradiente descendente para obtenção do mesmo resultado da função lm do R
theta0 = 0
theta1 = 0
theta  = c(theta0,theta1)
# Criando funções
Cost_computation <- function(x, y, theta){
# Verificação do número de exemplos de treinamento
m = length(y)
# Inicialização do Custo
J = 0
# Cômputo do custo a partir das informações fornecidas:
# i)   matriz de design
# ii)  rótulos ou respostas
# iii) parâmetros inicializados
# Parâmetros - de acordo com o modelo de regressão linear
Theta0 = theta[1]
Theta1 = theta[2]
# Função hipótese candidata de acordo com o modelo linear
h = Theta0 + Theta1*x
# Cômputo do custo (repare na versão vetorizada com Matlab)
Cost = sum((h - y)^2)
# Ponderação do custo pela quantidade de exemplos de treinamento
J = (1/(2*m))*Cost
}
# Cálculo da função custo para esses valores do vetor de parâmetros theta
Custo = Cost_computation(x,y,theta)
Custo
# Algoritmo do Gradiente Descendente ------------------------------------------------------------
# Inicializações relacionada ao gradiente descendente
num_iters     = 15000;
learning_rate = 0.01;
Algoritmo_GD <- function(X, y, theta, learning_rate, num_iters){
# Verificação do número de exemplos de treinamento
m = length(y)
# Uso da variável alpha = taxa de aprendizagem
alpha = learning_rate
# Loop para iterações do algoritmo GD
for (i in 1:num_iters){
# ================================================================================
h      = theta[1] + theta[2]*x                   # Função hipótese
Theta0 = theta[1]                                # Parâmetro (bias)
Theta1 = theta[2]                                # Parâmetro da característica
Theta0 = Theta0 - alpha*(1/m)*sum((h - y))       # Algoritmo GD (theta 0)
Theta1 = Theta1 - alpha*(1/m)*sum((h - y)*x)     # Algoritmo GD (theta 1)
theta  = c(Theta0, Theta1)                       # Composição de vetor de parâmetro
# print(theta)
}
# ================================================================================
theta
}
theta_GD <- Algoritmo_GD(x, y, theta, learning_rate, num_iters)
theta_GD
# ===============================================================================================
# Uso das equações normais para solução da regressão linear simples
# Repare que nós usamos a matriz de design com a característica unitária,
# pois queremos encontrar o parâmetro theta_0 ou intercept do modelo
# Característica unitária
size_data = dim(data)
ones_data = replicate(size_data[1],1)
# Matriz de Design
X = data.frame(ones_data,x)
X = as.matrix(X)
View(X)
theta_solution_norm_equations = solve(t(X) %*% X) %*% (t(X) %*% y)
theta_0_norm_eq = theta_solution_norm_equations[1]
theta_1_norm_eq = theta_solution_norm_equations[2]
theta_0_norm_eq
theta_1_norm_eq
summary(modelo_ML)
# ===============================================================================================
# Descrição detalhada do summary do modelo ------------------------------------------------------
# Summary do modelo - avaliando sua performance e detalhes
summary(modelo_ML)
# Equação de Regressão
# y = a + bx (simples)
# y = a + b0x0 + b1x1 (múltipla)
# Resíduos
# Diferença entre os valores observados de uma variável e seus valores previstos
# Seus resíduos devem se parecer com uma distribuição normal, o que indica
# que a média entre os valores previstos e os valores observados é próximo de 0 (o que é bom)
# Coeficiente - Intercept - a (alfa)
# Valor de a na equação de regressão
# Coeficientes - Nomes das variáveis - b (beta)
# Valor de b na equação de regressão
# Obs: A questão é que lm() ou summary() têm diferentes convenções de
# rotulagem para cada variável explicativa.
# Em vez de escrever slope_1, slope_2, ....
# Eles simplesmente usam o nome da variável em qualquer saída para
# indicar quais coeficientes pertencem a qual variável.
# Erro Padrão
# Medida de variabilidade na estimativa do coeficiente a (alfa). O ideal é que este valor
# seja menor que o valor do coeficiente, mas nem sempre isso irá ocorrer.
# Asteriscos
# Os asteriscos representam os níveis de significância de acordo com o p-value.
# Quanto mais estrelas, maior a significância.
# Atenção --> Muitos astericos indicam que é improvável que não exista
# relacionamento entre as variáveis.
# Valor t
# Define se coeficiente da variável é significativo ou não para o modelo.
# Ele é usado para calcular o p-value e os níveis de significância.
# p-value
# O p-value representa a probabilidade que a variável não seja relevante.
# Deve ser o menor valor possível.
# Se este valor for realmente pequeno, o R irá mostrar o valor
# como notação científica
# Significância
# São aquelas legendas próximas as suas variáveis
# Espaço em branco - ruim
# Pontos - razoável
# Asteriscos - bom
# Muitos asteriscos - muito bom
# Residual Standar Error
# Este valor representa o desvio padrão dos resíduos
# Degrees of Freedom
# É a diferença entre o número de observações na amostra de treinamento
# e o número de variáveis no seu modelo
# R-squared (coeficiente de determinação - R^2)
# Ajuda a avaliar o nível de precisão do nosso modelo.
# Quanto maior, melhor, sendo 1 o valor ideal.
# F-statistics
# É o teste F do modelo. Esse teste obtém os parâmetros do nosso modelo
# e compara com um modelo que tenha menos parâmetros.
# Em teoria, um modelo com mais parâmetros tem um desempenho melhor.
# Se o seu modelo com mais parâmetros NÃO tiver perfomance
# melhor que um modelo com menos parâmetros, o valor do p-value será bem alto.
# Se o modelo com mais parâmetros tiver performance
# melhor que um modelo com menos parâmetros, o valor do p-value será mais baixo.
# Lembre-se que correlação não implica causalidade
