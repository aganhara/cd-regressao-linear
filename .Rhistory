# Visualizando e acessando os coeficientes obtidos com o modelo
modelo_ML
# O que existe no modelo de ML?
names(modelo_ML)
# Coeficientes
modelo_ML$coefficients[1]
modelo_ML$coefficients[2]
# Res√≠duos do modelo (diferen√ßa entre o modelo e os dados de treinamento)
modelo_ML$residuals
# Obtenha a m√©dia dos res√≠duos
media_residuos = mean(modelo_ML$residuals)
media_residuos
# Gerando predi√ß√µes
?predict
previsao_treinamento = predict(modelo_ML)
class(previsao_treinamento)
previsao_treinamento = as.data.frame(previsao_treinamento)
View(previsao_treinamento)
class(previsao_treinamento)
# Plotando os dados de treinamento
plot(data$Population,data$Profit, main = 'Gr√°fico de Dispers√£o - Popula√ß√£o vs Lucro',
xlab = 'Popula√ß√£o (baseado 10,000 hab)', ylab = 'Lucro (baseado em $10,000 d√≥lares)', pch = 1, col = 'blue')
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
grid()
# Plotando a linha de regress√£o
?abline
# dica: fun√ß√£o abline(a,b)
# 1) a -> intercept do modelo de regress√£o
# 2) b -> coeficiente da reta do modelo de regress√£o
abline(modelo_ML$coefficients[1],modelo_ML$coefficients[2],col = 'red')
modelo_ML$coefficients[1]
# Modelo de regress√£o
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualiza√ß√£o
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=3, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regress√£o"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('Popula√ß√£o (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gr√°fico de Dispers√£o - Popula√ß√£o vs Lucro")
# Novo valor de popula√ß√£o (10 mil habitantes) - n√≥s n√£o sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(predicao_teste*10000)
names(predicao_teste) <- c('Predi√ß√µes (d√≥lares)')
predicao_teste
# ------------------------------------------------------------------------------------------------------
# Implementa√ß√£o do algoritmo do gradiente descendente para obten√ß√£o do mesmo resultado da fun√ß√£o lm do R
theta0 = 0
theta1 = 0
theta  = c(theta0,theta1)
# Criando fun√ß√µes
Cost_computation <- function(x, y, theta){
# Verifica√ß√£o do n√∫mero de exemplos de treinamento
m = length(y)
# Inicializa√ß√£o do Custo
J = 0
# C√¥mputo do custo a partir das informa√ß√µes fornecidas:
# i)   matriz de design
# ii)  r√≥tulos ou respostas
# iii) par√¢metros inicializados
# Par√¢metros - de acordo com o modelo de regress√£o linear
Theta0 = theta[1]
Theta1 = theta[2]
# Fun√ß√£o hip√≥tese candidata de acordo com o modelo linear
h = Theta0 + Theta1*x
# C√¥mputo do custo (repare na vers√£o vetorizada com Matlab)
Cost = sum((h - y)^2)
# Pondera√ß√£o do custo pela quantidade de exemplos de treinamento
J = (1/(2*m))*Cost
}
# C√°lculo da fun√ß√£o custo para esses valores do vetor de par√¢metros theta
Custo = Cost_computation(x,y,theta)
Custo
# Inicializa√ß√µes relacionada ao gradiente descendente
num_iters     = 15000;
learning_rate = 0.01;
Algoritmo_GD <- function(X, y, theta, learning_rate, num_iters){
# Verifica√ß√£o do n√∫mero de exemplos de treinamento
m = length(y)
# Uso da vari√°vel alpha = taxa de aprendizagem
alpha = learning_rate
# Loop para itera√ß√µes do algoritmo GD
for (i in 1:num_iters){
# ================================================================================
h      = theta[1] + theta[2]*x                   # Fun√ß√£o hip√≥tese
Theta0 = theta[1]                                # Par√¢metro (bias)
Theta1 = theta[2]                                # Par√¢metro da caracter√≠stica
Theta0 = Theta0 - alpha*(1/m)*sum((h - y))       # Algoritmo GD (theta 0)
Theta1 = Theta1 - alpha*(1/m)*sum((h - y)*x)     # Algoritmo GD (theta 1)
theta  = c(Theta0, Theta1)                       # Composi√ß√£o de vetor de par√¢metro
# print(theta)
}
# ================================================================================
theta
}
theta_GD <- Algoritmo_GD(x, y, theta, learning_rate, num_iters)
theta_GD
# Caracter√≠stica unit√°ria
size_data = dim(data)
ones_data = replicate(size_data[1],1)
# Matriz de Design
X = data.frame(ones_data,x)
X = as.matrix(X)
View(X)
theta_solution_norm_equations = solve(t(X) %*% X) %*% (t(X) %*% y)
theta_0_norm_eq = theta_solution_norm_equations[1]
theta_1_norm_eq = theta_solution_norm_equations[2]
theta_0_norm_eq
theta_1_norm_eq
# Summary do modelo - avaliando sua performance e detalhes
summary(modelo_ML)
# Equa√ß√£o de Regress√£o
# Equa√ß√£o de Regress√£o
# y = a + bx (simples)
# Res√≠duos
# Res√≠duos
# Diferen√ßa entre os valores observados de uma vari√°vel e seus valores previstos
# Res√≠duos
# Diferen√ßa entre os valores observados de uma vari√°vel e seus valores previstos
# Seus res√≠duos devem se parecer com uma distribui√ß√£o normal, o que indica
# Coeficiente - Intercept - a (alfa)
# Coeficientes - Nomes das vari√°veis - b (beta)
# Obs: A quest√£o √© que lm() ou summary() t√™m diferentes conven√ß√µes de
# Obs: A quest√£o √© que lm() ou summary() t√™m diferentes conven√ß√µes de
# rotulagem para cada vari√°vel explicativa.
# Obs: A quest√£o √© que lm() ou summary() t√™m diferentes conven√ß√µes de
# rotulagem para cada vari√°vel explicativa.
# Em vez de escrever slope_1, slope_2, ....
# Obs: A quest√£o √© que lm() ou summary() t√™m diferentes conven√ß√µes de
# rotulagem para cada vari√°vel explicativa.
# Em vez de escrever slope_1, slope_2, ....
# Eles simplesmente usam o nome da vari√°vel em qualquer sa√≠da para
x <- data$Population
y <- data$Profit
x <- data$Population
y <- data$Profit
# Plotando os dados de treinamento
plot(data$Population,data$Profit, main = 'Gr√°fico de Dispers√£o - Popula√ß√£o vs Lucro',
xlab = 'Popula√ß√£o (baseado 10,000 hab)', ylab = 'Lucro (baseado em $10,000 d√≥lares)', pch = 1, col = 'blue')
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
grid()
# Plotando a linha de regress√£o
?abline
# dica: fun√ß√£o abline(a,b)
# 1) a -> intercept do modelo de regress√£o
# 2) b -> coeficiente da reta do modelo de regress√£o
abline(modelo_ML$coefficients[1],modelo_ML$coefficients[2],col = 'red')
# Modelo de regress√£o
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualiza√ß√£o
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=3, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regress√£o"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('Popula√ß√£o (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gr√°fico de Dispers√£o - Popula√ß√£o vs Lucro")
# Modelo de regress√£o
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualiza√ß√£o
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=1, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regress√£o"),linetype = 1, size=1.5) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('Popula√ß√£o (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gr√°fico de Dispers√£o - Popula√ß√£o vs Lucro")
# Modelo de regress√£o
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualiza√ß√£o
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=1, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regress√£o"),linetype = 1, size=1) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('Popula√ß√£o (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gr√°fico de Dispers√£o - Popula√ß√£o vs Lucro")
# Modelo de regress√£o
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualiza√ß√£o
ggplot(data,
aes(x = Population, y = Profit)) +
geom_point(size=1.5, aes(colour = "Dados de Treinamento")) +
geom_line(aes(x = x, y = h, colour = "Modelo de Regress√£o"),linetype = 1, size=1) +
scale_colour_manual(name="Legenda", values=c("blue", "red")) +
xlab('Popula√ß√£o (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
ggtitle("Gr√°fico de Dispers√£o - Popula√ß√£o vs Lucro")
# Novo valor de popula√ß√£o (10 mil habitantes) - n√≥s n√£o sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(predicao_teste*10000)
names(predicao_teste) <- c('Predi√ß√µes (d√≥lares)')
predicao_teste
# Novo valor de popula√ß√£o (10 mil habitantes) - n√≥s n√£o sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(novo_dado, predicao_teste*10000)
names(predicao_teste) <- c('Popula√ß√£o','Predi√ß√µes (d√≥lares)')
predicao_teste
# Novo valor de popula√ß√£o (10 mil habitantes) - n√≥s n√£o sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(novo_dado*10000, predicao_teste*10000)
names(predicao_teste) <- c('Popula√ß√£o','Predi√ß√µes (d√≥lares)')
predicao_teste
# Implementa√ß√£o do algoritmo do gradiente descendente para obten√ß√£o do mesmo resultado da fun√ß√£o lm do R
theta0 = 0
theta1 = 0
theta  = c(theta0,theta1)
# Criando fun√ß√µes
Cost_computation <- function(x, y, theta){
# Verifica√ß√£o do n√∫mero de exemplos de treinamento
m = length(y)
# Inicializa√ß√£o do Custo
J = 0
# C√¥mputo do custo a partir das informa√ß√µes fornecidas:
# i)   matriz de design
# ii)  r√≥tulos ou respostas
# iii) par√¢metros inicializados
# Par√¢metros - de acordo com o modelo de regress√£o linear
Theta0 = theta[1]
Theta1 = theta[2]
# Fun√ß√£o hip√≥tese candidata de acordo com o modelo linear
h = Theta0 + Theta1*x
# C√¥mputo do custo (repare na vers√£o vetorizada com Matlab)
Cost = sum((h - y)^2)
# Pondera√ß√£o do custo pela quantidade de exemplos de treinamento
J = (1/(2*m))*Cost
}
# C√°lculo da fun√ß√£o custo para esses valores do vetor de par√¢metros theta
Custo = Cost_computation(x,y,theta)
Custo
# Algoritmo do Gradiente Descendente ------------------------------------------------------------
# Inicializa√ß√µes relacionada ao gradiente descendente
num_iters     = 15000;
learning_rate = 0.01;
Algoritmo_GD <- function(X, y, theta, learning_rate, num_iters){
# Verifica√ß√£o do n√∫mero de exemplos de treinamento
m = length(y)
# Uso da vari√°vel alpha = taxa de aprendizagem
alpha = learning_rate
# Loop para itera√ß√µes do algoritmo GD
for (i in 1:num_iters){
# ================================================================================
h      = theta[1] + theta[2]*x                   # Fun√ß√£o hip√≥tese
Theta0 = theta[1]                                # Par√¢metro (bias)
Theta1 = theta[2]                                # Par√¢metro da caracter√≠stica
Theta0 = Theta0 - alpha*(1/m)*sum((h - y))       # Algoritmo GD (theta 0)
Theta1 = Theta1 - alpha*(1/m)*sum((h - y)*x)     # Algoritmo GD (theta 1)
theta  = c(Theta0, Theta1)                       # Composi√ß√£o de vetor de par√¢metro
# print(theta)
}
# ================================================================================
theta
}
theta_GD <- Algoritmo_GD(x, y, theta, learning_rate, num_iters)
theta_GD
# ===============================================================================================
# Uso das equa√ß√µes normais para solu√ß√£o da regress√£o linear simples
# Repare que n√≥s usamos a matriz de design com a caracter√≠stica unit√°ria,
# pois queremos encontrar o par√¢metro theta_0 ou intercept do modelo
# Caracter√≠stica unit√°ria
size_data = dim(data)
ones_data = replicate(size_data[1],1)
# Matriz de Design
X = data.frame(ones_data,x)
X = as.matrix(X)
View(X)
theta_solution_norm_equations = solve(t(X) %*% X) %*% (t(X) %*% y)
theta_0_norm_eq = theta_solution_norm_equations[1]
theta_1_norm_eq = theta_solution_norm_equations[2]
theta_0_norm_eq
theta_1_norm_eq
summary(modelo_ML)
# ===============================================================================================
# Descri√ß√£o detalhada do summary do modelo ------------------------------------------------------
# Summary do modelo - avaliando sua performance e detalhes
summary(modelo_ML)
# Equa√ß√£o de Regress√£o
# y = a + bx (simples)
# y = a + b0x0 + b1x1 (m√∫ltipla)
# Res√≠duos
# Diferen√ßa entre os valores observados de uma vari√°vel e seus valores previstos
# Seus res√≠duos devem se parecer com uma distribui√ß√£o normal, o que indica
# que a m√©dia entre os valores previstos e os valores observados √© pr√≥ximo de 0 (o que √© bom)
# Coeficiente - Intercept - a (alfa)
# Valor de a na equa√ß√£o de regress√£o
# Coeficientes - Nomes das vari√°veis - b (beta)
# Valor de b na equa√ß√£o de regress√£o
# Obs: A quest√£o √© que lm() ou summary() t√™m diferentes conven√ß√µes de
# rotulagem para cada vari√°vel explicativa.
# Em vez de escrever slope_1, slope_2, ....
# Eles simplesmente usam o nome da vari√°vel em qualquer sa√≠da para
# indicar quais coeficientes pertencem a qual vari√°vel.
# Erro Padr√£o
# Medida de variabilidade na estimativa do coeficiente a (alfa). O ideal √© que este valor
# seja menor que o valor do coeficiente, mas nem sempre isso ir√° ocorrer.
# Asteriscos
# Os asteriscos representam os n√≠veis de signific√¢ncia de acordo com o p-value.
# Quanto mais estrelas, maior a signific√¢ncia.
# Aten√ß√£o --> Muitos astericos indicam que √© improv√°vel que n√£o exista
# relacionamento entre as vari√°veis.
# Valor t
# Define se coeficiente da vari√°vel √© significativo ou n√£o para o modelo.
# Ele √© usado para calcular o p-value e os n√≠veis de signific√¢ncia.
# p-value
# O p-value representa a probabilidade que a vari√°vel n√£o seja relevante.
# Deve ser o menor valor poss√≠vel.
# Se este valor for realmente pequeno, o R ir√° mostrar o valor
# como nota√ß√£o cient√≠fica
# Signific√¢ncia
# S√£o aquelas legendas pr√≥ximas as suas vari√°veis
# Espa√ßo em branco - ruim
# Pontos - razo√°vel
# Asteriscos - bom
# Muitos asteriscos - muito bom
# Residual Standar Error
# Este valor representa o desvio padr√£o dos res√≠duos
# Degrees of Freedom
# √â a diferen√ßa entre o n√∫mero de observa√ß√µes na amostra de treinamento
# e o n√∫mero de vari√°veis no seu modelo
# R-squared (coeficiente de determina√ß√£o - R^2)
# Ajuda a avaliar o n√≠vel de precis√£o do nosso modelo.
# Quanto maior, melhor, sendo 1 o valor ideal.
# F-statistics
# √â o teste F do modelo. Esse teste obt√©m os par√¢metros do nosso modelo
# e compara com um modelo que tenha menos par√¢metros.
# Em teoria, um modelo com mais par√¢metros tem um desempenho melhor.
# Se o seu modelo com mais par√¢metros N√ÉO tiver perfomance
# melhor que um modelo com menos par√¢metros, o valor do p-value ser√° bem alto.
# Se o modelo com mais par√¢metros tiver performance
# melhor que um modelo com menos par√¢metros, o valor do p-value ser√° mais baixo.
# Lembre-se que correla√ß√£o n√£o implica causalidade
- CRIM: per capita crime rate by town
- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.
- INDUS: proportion of non-retail business acres per town
- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
- NOX: nitric oxides concentration (parts per 10 million)
- RM: average number of rooms per dwelling
- AGE: proportion of owner-occupied units built prior to 1940
- DIS: weighted distances to five Boston employment centres
- RAD: index of accessibility to radial highways
- TAX: full-value property-tax rate per 10,000
- PTRATIO: pupil-teacher ratio by town
- B: 1000(BkÙÄÄÄ0:63)2 where Bk is the proportion of blacks by town
- LSTAT: % lower status of the population
- TARGET: Median value of owner-occupied homes in $1000‚ÄôsEm um primeiro momento, vamos usar a vari√°vel explanat√≥ria lstat, a qual expressa a parcela (em %) da
popula√ß√£o de baixa renda (status) obtida em cada vizinhan√ßa dentre as 506 analisadas.
O nosso objetivo consiste em obter um modelo de ML baseado na regress√£o linear simples e univariada de
lstat e os valores medianos dos pre√ßos das casas medv. Abaixo, seguem os itens que devemos solucionar neste
desenvolvimento, visando alcan√ßar o objetivo deste exerc√≠cio:
 Quest√µes Avaliativas
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
data
data
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
?Boston
data <- Boston
data
data <- Boston
library(MASS)
?Boston
data <- Boston
data
data <- Boston
data
head(data)
head(data)
install.packages(c("backports", "broom", "callr", "car", "carData", "dbplyr", "dplyr", "ellipsis", "foreach", "forecast", "fs", "ggplot2", "glue", "haven", "httpuv", "isoband", "janitor", "later", "lme4", "lubridate", "maptools", "modelr", "openxlsx", "padr", "pillar", "pkgbuild", "pkgload", "plotly", "pROC", "promises", "ps", "purrr", "quantmod", "Rcpp", "RcppArmadillo", "recipes", "reshape2", "rlang", "rmarkdown", "scales", "seasonal", "shinyWidgets", "sp", "SQUAREM", "tibble", "tibbletime", "tidyr", "tidyselect", "timetk", "tinytex", "tripack", "vctrs", "withr", "xfun", "xml2", "zoo"))
knitr::opts_chunk$set(echo = TRUE)
data <- Boston
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
library(MASS)
?Boston
data <- Boston
head(data)
knitr::opts_chunk$set(echo = TRUE)
plot(cars)
library(MASS)
?Boston
summary(Boston)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
# Utiliza fun√ß√µes gr√°ficas simples para verificar a rela√ß√£o entre vari√°veis:
# ?plot
plot(data$lstat, data$medv, main = 'Gr√°fico de Dispers√£o - lstat vs medv',
xlab = 'lstat', ylab = 'Pre√ßo mediano da casa', pch = 1, col = 'blue')
# ?legend
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1))
# ?grid
grid()
modelo_ML <- lm(medv ~ lstat, data)
x <- data$lstat
y <- data$medv
summary(modelo_ML)
# Coeficientes
modelo_ML$coefficients[1]
modelo_ML$coefficients[2]
# Res√≠duos do modelo (diferen√ßa entre o modelo e os dados de treinamento)
modelo_ML$residuals
# Obtenha a m√©dia dos res√≠duos
media_residuos = mean(modelo_ML$residuals)
media_residuos
previsao_treinamento <- predict(modelo_ML) %>% as.data.frame()
library(MASS)
library(rlang)
library(tidyverse)
install.packages(rlang)
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rlang)
library(tidyverse)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rlang)
library(tidyverse)
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
saveRDS(df,'C:\Users\aganh\Documents\Ci√™ncia de dados e decis√£o\Exerc√≠cios computacionais\cd-regressao-linear\.RData')
saveRDS(df,'C:\\Users\\aganh\\Documents\\Ci√™ncia de dados e decis√£o\\Exerc√≠cios computacionais\\cd-regressao-linear\\.RData')
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
