# ===============================================================================================
# Curso: Introdução à Ciência de Dados e Decisões 


# Aula  - Modelos de Regressão Linear Simples e Múltipla

# Professor - Ricardo Augusto


# =========================
# Exercicio Computacional 1

# Objetivo: implementar modelos de regressão linear simples no R


# --------------------------------------------------------------
# Carregamento de pacotes a serem usados e comandos iniciais
library(ggplot2)


# Configurar o diretório atual usando setwd
setwd('C:/Users/1513 X-MXTI/Desktop/Projeto 4I Machine Learning Classes/Aula 2 - Regressão Linear/2 - Lista de Atividades/Exercício Computacional/Exercício Computacional - 1/R')
getwd() # verifique o diretório atual 


# Use o comando list.files() para verificar os arquivos presentes no diretório
list.files()

# Nesse ponto, você deverá os arquivos presentes no seu diretório. 
# É importante que você coloque a pequena base de dados enviada nesse diretório

# ------------------------------------------------------------
# Carregamento do Dataset de exemplo 

# Com o arquivo de dados presente no diretório - vamos carregá-lo com read.table
data <- read.table('data.txt', sep = ',')

# Reparem que eu utilizei o operador de separação (sep) com o valor de vírgula (,)
# Isso significa que o limitador usado é baseado nesse caractere (,). 

data
class(data)    # Toda vez que importarmos algum conjunto de dados para o R, verifique sua classe (data.frame)
dim(data)      # Verificar a dimensão do objeto data
View(data)     # Use essa instrução para visualizar os dados (tal como um spreedcheat do excel)

# Nomeando as colunas de um dataframe - vamos usar a instrução names para colocar nomes nas colunas
names(data) <- c("Population","Profit")
head(data)
View(data)
# Repare que podemos explorar mais argumentos da função read.table para fazer a leitura dos cabeçalhos do data set


# ===============================================================================================
# Exploração de Dados ---------------------------------------------------------------------------

# Nessa fase, estamos interessados em usar o aprendizado das aulas anteriores para fazer nossas explorações
# O dataset é simples e reúne dados populacionais e de lucro de diversas empresas

# Vamos fazer gráficos com:
# i)  instruções do R base -  pacote gráfico básico do R
# ii) instruções do ggplot2 - pacote mais sofisticado do R


# Utiliza funções gráficas simples para verificar a relação entre variáveis: população (population) e lucro (profit)
# ?plot
plot(data$Population, data$Profit, main = 'Gráfico de Dispersão - População vs Lucro', 
     xlab = 'População', ylab = 'Lucro', pch = 1, col = 'blue')
# ?legend
legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1)) 
# ?grid 
grid()

# Menor lucro
min(data$Profit)
max(data$Profit)

# Dicas: 
# 1) é sempre recomendável deixar "comentado" o comando de help das funções que estamos usando (objetivo de familiarização de parâmetros)
# 2) reparem que estamos usando o operador $ e ~ são duas formas diferentes de relacionar as variáveis medv e lstat
# 3) usamos o main, xlab e ylab para atribuir os rótulos que precisamos aos gráficos
# 4) o parâmetro pch nos permite especificar o símbolo que iremos usar para o plot
# 5) o parâmetro col nos permite diretamente alterar a cor dos símbolos diretamente
# 6) usamos esse gráfico de pontos (i.e., dispersão) para entender a relação entre variáveis

# Uso do pacote ggplot2 para traçar os gráficos

# Construção de Camadas com ggplot2
objeto_ggplot <- ggplot(data,
                        aes( x = Population, y = Profit)) + 
                        xlab('População (baseada em 10.000 habitantes)')+
                        ylab('Lucro (baseado em $10,000 dólares)')
  
# Criação de uma camada de pontos                        
camada_1 <- geom_point()


# Geração do gráfico a adição da camada 1
objeto_ggplot + camada_1


# ===============================================================================================
# Modelagem Preditiva - Regressão Linear Simples ------------------------------------------------

modelo_ML <- lm(Profit ~ Population, data = data)
modelo_ML

# Dicas: 
# 1) reparem que o uso de uma única variável característica (também chamada de preditora) consiste em uma regressão linear simples
# 2) o uso de múltiplas variáveis características (preditoras) consiste na regressão linear múltipla
# 3) o uso do operador ~ indica a relação entre variáveis, sendo que a variável de saída é posicionada à esquerda de ~, enquanto a variável explanatória é posicionada à direita
# 4) a função lm utiliza a decomposição QR para solucionar o problema de regressão linear 
# 5) principal referência: Chambers, J. M. (1992) Linear models. Chapter 4 of Statistical Models in S eds J. M. Chambers and T. J. Hastie, Wadsworth & Brooks/Cole.

x = data$Population
y = data$Profit
# dados = data.frame(x,y)
# dados
# modelo_ML_2 <- lm(y ~ x, data = dados)
# modelo_ML_2

# Do lado esquerdo de ~ temos a variável alvo (target)
# Do lado direito de ~ temos as características

# --------------------------------------------------------------------------------------------
# Verificando os resultados do modelo de ML com a função lm e comparando resultados

# Visualizando e acessando os coeficientes obtidos com o modelo
modelo_ML
# O que existe no modelo de ML?
names(modelo_ML)

# Coeficientes
modelo_ML$coefficients[1]
modelo_ML$coefficients[2]

# Resíduos do modelo (diferença entre o modelo e os dados de treinamento)
modelo_ML$residuals

# Obtenha a média dos resíduos
media_residuos = mean(modelo_ML$residuals)
media_residuos

# Dica: é interessante verificar a diferença entre as palavras erro e resíduo

# Gerando predições 
?predict
previsao_treinamento = predict(modelo_ML)
class(previsao_treinamento)
previsao_treinamento = as.data.frame(previsao_treinamento)
View(previsao_treinamento)
class(previsao_treinamento)
# colnames(previsão_treinamento) <- 'Pred'

# Dica 
# 1) Essas são predições geradas pelo modelo para os dados de treinamento
# 2) Nós usamos o modelo treinado e geramos as predições a partir dos dados que nós temos para treino
# 3) Fazer predições sobre novos dados -> predict(modelo_ML, dados_teste)

# ===============================================================================================
# Plotando as linhas de regressão ---------------------------------------------------------------

# Plotando os dados de treinamento 
plot(data$Population,data$Profit, main = 'Gráfico de Dispersão - População vs Lucro', 
     xlab = 'População (baseado 10,000 hab)', ylab = 'Lucro (baseado em $10,000 dólares)', pch = 1, col = 'blue')

legend("topright",legend = c('Dados'), col = c('blue'), pch = c(1)) 
grid() 

# Plotando a linha de regressão
?abline
# dica: função abline(a,b)
# 1) a -> intercept do modelo de regressão
# 2) b -> coeficiente da reta do modelo de regressão
abline(modelo_ML$coefficients[1],modelo_ML$coefficients[2],col = 'red')

# Modelo de regressão
h = modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*x
# Visualização
ggplot(data, 
       aes(x = Population, y = Profit)) + 
  geom_point(size=3, aes(colour = "Dados de Treinamento")) + 
  geom_line(aes(x = x, y = h, colour = "Modelo de Regressão"),linetype = 1, size=1.5) +
  scale_colour_manual(name="Legenda", values=c("blue", "red")) + 
  xlab('População (baseada em 10.000)') + ylab('Lucro (baseado em $10.000)')  +
  ggtitle("Gráfico de Dispersão - População vs Lucro")


# ===============================================================================================
# Predição do Modelo para novos dados ------------------------------------------------------

# Novo valor de população (10 mil habitantes) - nós não sabemos a previsibilidade para esse valor de entrada
novo_dado      = data.frame(c(10, 12, 14, 16, 18, 20))
colnames(novo_dado) <- c('Population')
novo_dado
predicao_teste <- predict(modelo_ML, novo_dado)
predicao_teste <- data.frame(predicao_teste*10000)
names(predicao_teste) <- c('Predições (dólares)')
predicao_teste

# h_pred <- modelo_ML$coefficients[1] + modelo_ML$coefficients[2]*10
# h_pred <- h_pred*10000
# h_pred

# ------------------------------------------------------------------------------------------------------
# Implementação do algoritmo do gradiente descendente para obtenção do mesmo resultado da função lm do R
theta0 = 0
theta1 = 0
theta  = c(theta0,theta1)
# Criando funções
Cost_computation <- function(x, y, theta){
  
  # Verificação do número de exemplos de treinamento 
  m = length(y)
  
  # Inicialização do Custo
  J = 0
  
  # Cômputo do custo a partir das informações fornecidas:
  # i)   matriz de design
  # ii)  rótulos ou respostas
  # iii) parâmetros inicializados
  
  # Parâmetros - de acordo com o modelo de regressão linear
  Theta0 = theta[1]
  Theta1 = theta[2]
  
  # Função hipótese candidata de acordo com o modelo linear
  h = Theta0 + Theta1*x    
  
  # Cômputo do custo (repare na versão vetorizada com Matlab)
  Cost = sum((h - y)^2)
  
  # Ponderação do custo pela quantidade de exemplos de treinamento
  J = (1/(2*m))*Cost
}

# Cálculo da função custo para esses valores do vetor de parâmetros theta
Custo = Cost_computation(x,y,theta)
Custo

# -----------------------------------------------------------------------------------------------
# Algoritmo do Gradiente Descendente ------------------------------------------------------------

# Inicializações relacionada ao gradiente descendente
num_iters     = 15000;
learning_rate = 0.01;

Algoritmo_GD <- function(X, y, theta, learning_rate, num_iters){
  
  # Verificação do número de exemplos de treinamento 
  m = length(y)
  
  # Uso da variável alpha = taxa de aprendizagem 
  alpha = learning_rate
  
  # Loop para iterações do algoritmo GD
  for (i in 1:num_iters){
    # ================================================================================
    h      = theta[1] + theta[2]*x                   # Função hipótese 
    Theta0 = theta[1]                                # Parâmetro (bias)
    Theta1 = theta[2]                                # Parâmetro da característica
    Theta0 = Theta0 - alpha*(1/m)*sum((h - y))       # Algoritmo GD (theta 0) 
    Theta1 = Theta1 - alpha*(1/m)*sum((h - y)*x)     # Algoritmo GD (theta 1)
    theta  = c(Theta0, Theta1)                       # Composição de vetor de parâmetro
    # print(theta)
  }                         
  # ================================================================================
  theta
}
theta_GD <- Algoritmo_GD(x, y, theta, learning_rate, num_iters)
theta_GD

# ===============================================================================================
# Uso das equações normais para solução da regressão linear simples
# Repare que nós usamos a matriz de design com a característica unitária,
# pois queremos encontrar o parâmetro theta_0 ou intercept do modelo

# Característica unitária
size_data = dim(data)
ones_data = replicate(size_data[1],1)

# Matriz de Design
X = data.frame(ones_data,x)
X = as.matrix(X)
View(X)

theta_solution_norm_equations = solve(t(X) %*% X) %*% (t(X) %*% y) 
theta_0_norm_eq = theta_solution_norm_equations[1]
theta_1_norm_eq = theta_solution_norm_equations[2]
theta_0_norm_eq
theta_1_norm_eq

# ===============================================================================================
# Descrição detalhada do summary do modelo ------------------------------------------------------

# Summary do modelo - avaliando sua performance e detalhes
summary(modelo_ML)

# Equação de Regressão
# y = a + bx (simples)
# y = a + b0x0 + b1x1 (múltipla)

# Resíduos
# Diferença entre os valores observados de uma variável e seus valores previstos
# Seus resíduos devem se parecer com uma distribuição normal, o que indica
# que a média entre os valores previstos e os valores observados é próximo de 0 (o que é bom)

# Coeficiente - Intercept - a (alfa)
# Valor de a na equação de regressão

# Coeficientes - Nomes das variáveis - b (beta)
# Valor de b na equação de regressão

# Obs: A questão é que lm() ou summary() têm diferentes convenções de 
# rotulagem para cada variável explicativa. 
# Em vez de escrever slope_1, slope_2, .... 
# Eles simplesmente usam o nome da variável em qualquer saída para 
# indicar quais coeficientes pertencem a qual variável.

# Erro Padrão
# Medida de variabilidade na estimativa do coeficiente a (alfa). O ideal é que este valor 
# seja menor que o valor do coeficiente, mas nem sempre isso irá ocorrer.

# Asteriscos 
# Os asteriscos representam os níveis de significância de acordo com o p-value.
# Quanto mais estrelas, maior a significância.
# Atenção --> Muitos astericos indicam que é improvável que não exista 
# relacionamento entre as variáveis.

# Valor t
# Define se coeficiente da variável é significativo ou não para o modelo. 
# Ele é usado para calcular o p-value e os níveis de significância.

# p-value
# O p-value representa a probabilidade que a variável não seja relevante. 
# Deve ser o menor valor possível. 
# Se este valor for realmente pequeno, o R irá mostrar o valor 
# como notação científica

# Significância
# São aquelas legendas próximas as suas variáveis
# Espaço em branco - ruim
# Pontos - razoável
# Asteriscos - bom
# Muitos asteriscos - muito bom

# Residual Standar Error
# Este valor representa o desvio padrão dos resíduos

# Degrees of Freedom
# É a diferença entre o número de observações na amostra de treinamento 
# e o número de variáveis no seu modelo

# R-squared (coeficiente de determinação - R^2)
# Ajuda a avaliar o nível de precisão do nosso modelo. 
# Quanto maior, melhor, sendo 1 o valor ideal.

# F-statistics
# É o teste F do modelo. Esse teste obtém os parâmetros do nosso modelo 
# e compara com um modelo que tenha menos parâmetros.
# Em teoria, um modelo com mais parâmetros tem um desempenho melhor. 

# Se o seu modelo com mais parâmetros NÃO tiver perfomance
# melhor que um modelo com menos parâmetros, o valor do p-value será bem alto. 

# Se o modelo com mais parâmetros tiver performance
# melhor que um modelo com menos parâmetros, o valor do p-value será mais baixo.

# Lembre-se que correlação não implica causalidade

