"0","# Algoritmo do Gradiente Descendente ------------------------------------------------------------"
"0",""
"0","# Inicializações relacionada ao gradiente descendente"
"0","num_iters     = 15000;"
"0","learning_rate = 0.01;"
"0",""
"0","Algoritmo_GD <- function(X, y, theta, learning_rate, num_iters){"
"0","  "
"0","  # Verificação do número de exemplos de treinamento "
"0","  m = length(y)"
"0","  "
"0","  # Uso da variável alpha = taxa de aprendizagem "
"0","  alpha = learning_rate"
"0","  "
"0","  # Loop para iterações do algoritmo GD"
"0","  for (i in 1:num_iters){"
"0","    # ================================================================================"
"0","    h      = theta[1] + theta[2]*x                   # Função hipótese "
"0","    Theta0 = theta[1]                                # Parâmetro (bias)"
"0","    Theta1 = theta[2]                                # Parâmetro da característica"
"0","    Theta0 = Theta0 - alpha*(1/m)*sum((h - y))       # Algoritmo GD (theta 0) "
"0","    Theta1 = Theta1 - alpha*(1/m)*sum((h - y)*x)     # Algoritmo GD (theta 1)"
"0","    theta  = c(Theta0, Theta1)                       # Composição de vetor de parâmetro"
"0","    # print(theta)"
"0","  }                         "
"0","  # ================================================================================"
"0","  theta"
"0","}"
"0","theta_GD <- Algoritmo_GD(x, y, theta, learning_rate, num_iters)"
"0","theta_GD"
"1","[1]"
"1"," -3.895781"
"1","  1.193034"
"1","
"
